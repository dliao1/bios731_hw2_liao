---
title: "P3"
output: html_document
date: "2025-02-12"
---

```{r, echo= FALSE, include = FALSE}
library(tidyverse)
library(here)
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 5
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

source(here("source", "utils.R"))
set.seed(3000)
```

```{r}
newton_logit <- function(beta = c(0,0), x, y, tol = 1e-8, max_iter = 100) {

  beta_cur = beta
  beta_history = gradient_vec = matrix(NA, nrow = max_iter, 
                                       ncol = length(beta))
  for (iter in 1:max_iter) {
    
    # store results
    beta_history[iter,] = beta_cur
    
    # Compute the gradient and hessian
    new_pi <- exp(x %*% beta_cur) / (1 + exp(x %*% beta_cur))
    
    gradient <- t(x) %*% (y - new_pi)
    hessian_ls = as.list(rep(NA, length.out = length(y)))
    
    for(i in 1:length(y)){
      hessian_ls[[i]] <- as.numeric(new_pi[i, ] * (1 - new_pi[i, ])) * tcrossprod(x[i,], x[i,])
    }
    
    hessian <- -1 * Reduce("+", hessian_ls)
    
    gradient_vec[iter,] = gradient
    
    # Change stopping criterion ?? either converges super fast or not at all..
    if(sqrt(sum(gradient^2)) < tol){
      message("Converged in", iter, "iterations.\n")
      print(solve(hessian)) # is this the variance...
      break
    }
    
    # Update the solution
    beta_cur = beta_cur - solve(hessian) %*% gradient
  }
  
  return(list(solution = beta_cur, 
              beta_history = beta_history,
              gradient = gradient_vec,
              converged = (iter < max_iter),
              niter = iter))
}
```



```{r}


sim_data <- gen_logit_data(n = 200,
                     beta0 = 1,
                     beta1 = 0.3)

initial_guess <- c(2, 1)

test = newton_logit(beta = initial_guess,
                    x = sim_data$x,
                    y = sim_data$y)

test$solution

mod = glm(y ~ x, data = sim_data, family = binomial)
coef(mod)

```
