---
title: "HW2"
output:
  pdf_document: default
  html_document: default
date: "2025-02-12"
---

# TODO:

Put 2A and 2C in on Latex
implement MM (uh oh)
# Setup

```{r, echo= FALSE, include = FALSE}
library(tidyverse)
library(here)
library(survival)
library(eha)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 5
)

theme_set(theme_bw() + theme(legend.position = "bottom"))

source(here("source", "utils.R"))
set.seed(3000)
```

# 1
Let $\pi_i = P(Y_i = 1 \mid X_i = x_i), Y_i \in \{0,1\}$. The logistic regression model is given by:

$\text{logit}(\pi_i) = \log \left( \frac{\pi_i}{1 - \pi_i} \right) = \log \left( \frac{P(Y_i = 1 \mid X_i)}{1 - P(Y_i = 1 \mid X_i)} \right) = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip} = X_i^T \beta$

where $\beta$ is a $p \times 1$ vector.

The response variables follow a Bernoulli distribution: $Y_1, Y_2, \dots, Y_n \sim \text{Bern}(\pi)$, with probability mass function:

$f(y_i \mid \pi_i) = \pi_i^{y_i} (1 - \pi_i)^{1 - y_i}$

The expectation of $Y_i$ given $X_i$ is $E[Y_i \mid X_i] = \pi_i$. Since $\log \left( \frac{E[Y_i \mid X_i]}{1 - E[Y_i \mid X_i]} \right) = X_i^T \beta$, we exponentiate both sides:

$e^{X_i^T \beta} = \frac{E[Y_i \mid X_i]}{1 - E[Y_i \mid X_i]}$

$e^{X_i^T \beta} - E[Y_i \mid X_i]e^{X_i^T \beta} = E[Y_i \mid X_i]$ 

$e^{X_i^T \beta} = E[Y_i \mid X_i] + E[Y_i \mid X_i]e^{X_i^T \beta}$ 


$e^{X_i^T \beta} = E[Y_i \mid X_i] (1 + e^{X_i^T \beta})$

$\frac{e^{X_i^T \beta}}{1 + e^{X_i^T \beta}} = E[Y_i \mid X_i] = \pi_i$

## Likelihood

The likelihood function is:

$L(\beta) = \prod_{i=1}^{n} \pi_i^{y_i} (1 - \pi_i)^{1 - y_i}$

Substituting $\pi_i = \frac{e^{X_i^T \beta}}{1 + e^{X_i^T \beta}}$

$$L(\beta) = \prod_{i=1}^{n} \left( \frac{e^{X_i^T \beta}}{1 + e^{X_i^T \beta}} \right)^{y_i} \left( \frac{1}{1 + e^{X_i^T \beta}} \right)^{1 - y_i}$$


The log-likelihood function is given by:

$\ell(\beta) = \log L(\beta) = \log \prod_{i=1}^{n} \left( \frac{e^{X_i^T \beta}}{1 + e^{X_i^T \beta}} \right)^{y_i} \left( 1 - \frac{e^{X_i^T \beta}}{1 + e^{X_i^T \beta}} \right)^{1 - y_i}$

$\ell(\beta) = \sum_{i=1}^{n} \left[ y_i \log \left( \frac{e^{X_i^T \beta}}{1 + e^{X_i^T \beta}} \right) + (1 - y_i) \log \left( 1 - \frac{e^{X_i^T \beta}}{1 + e^{X_i^T \beta}} \right) \right]$

$\sum_{i=1}^{n} \left[ y_i (\log e^{X_i^T \beta} - \log(1 + e^{X_i^T \beta})) + (1 - y_i) \left ( \log \frac{1}{1 + e^{X_i^T \beta}} \right ) \right]$

$\sum_{i=1}^{n} \left[ y_i ({X_i^T \beta} - \log(1 + e^{X_i^T \beta})) + (1 - y_i) (\log 1 - \log(1 + e^{X_i^T \beta})) \right]$

$\sum_{i=1}^{n} \left[ y_i X_i^T \beta - \log(1 + e^{X_i^T \beta}) \right]$

Thus, the final form of the log-likelihood function is:

$\ell(\beta) = \sum_{i=1}^{n} y_i X_i^T \beta - \sum_{i=1}^{n} \log(1 + e^{X_i^T \beta})$

## Gradient

$\frac{\partial \ell}{\partial \beta} = \sum_{i=1}^{n} y_i X_i^T - \sum_{i=1}^{n} \frac{X_i e^{X_i^T \beta}}{1 + e^{X_i^T \beta}}$

Factoring common terms out:
$\frac{\partial \ell}{\partial \beta} = \sum_{i=1}^{n} \left (y_i - \frac{e^{X_i^T \beta}}{1 + e^{X_i^T \beta}} \right ) X_i$

Rewriting in terms of $\pi_i$ we get:

$\frac{\partial \ell}{\partial \beta} = \sum_{i=1}^{n} (y_i - \pi_i) X_i$

## Hessian 

$\frac{\partial^2 \ell}{\partial \beta^2} = - \sum_{i=1}^{n} X_i^T X_i \frac{e^{X_i^T \beta} (1 + e^{X_i^T \beta}) - e^{2X_i^T \beta}}{(1 + e^{X_i^T \beta})^2}$

Simplifying:
$\frac{\partial^2 \ell}{\partial \beta^2} = - \sum_{i=1}^{n}  \frac{X_i^T X_i e^{X_i^T \beta} [(1 + e^{X_i^T \beta}) - e^{X_i^T \beta}]}{(1 + e^{X_i^T \beta})^2}$

$\frac{\partial^2 \ell}{\partial \beta^2} = - \sum_{i=1}^{n} X_i^T X_i \frac{e^{X_i^T \beta}}{(1 + e^{X_i^T \beta})^2}$

Since:

$\pi_i (1 - \pi_i) = \frac{e^{X_i^T \beta}}{(1 + e^{X_i^T \beta})^2}$

$\frac{\partial^2 \ell}{\partial \beta^2} = - \sum_{i=1}^{n} X_i^T (\pi_i (1 - \pi_i)) X_i$

## Newton's Method Update

$\beta_{t+1} = \beta_t - \left ( \frac{\partial^2 \ell}{\partial \beta^2} \right )^{-1} \frac{\partial \ell}{\partial \beta}$

$\beta_{t+1} = \beta_t - \left ( \frac{\sum_{i=1}^{n} (y_i - \pi_i) X_i} { - \sum_{i=1}^{n} X_i^T (\pi_i (1 - \pi_i)) X_i} \right )$

where  $\pi_i = \frac{e^{X_i^T \beta}}{1 + e^{X_i^T \beta}}$

## Convex Optimization

Gradient of -log f is the negative Hessian:

$-\frac{\partial^2 \ell} {\partial \beta^2} = + \sum_{i=1}^{n} X_i^T (\pi_i (1 - \pi_i)) X_i \geq 0$

The negative hessian is always positive, thus this is a convex optimization problem.

# 2

Add in Latex here

# 3
## Newton
```{r}
newton_logit <- function(beta, x, y, tol = 1e-8, max_iter = 100) {

  beta_cur = beta
  beta_history = gradient_vec = matrix(NA, nrow = max_iter, 
                                       ncol = length(beta))
  se_beta <- NA
  for (iter in 1:max_iter) {
    
    # store results
    beta_history[iter,] = beta_cur
    
    # Compute the gradient and hessian
    new_pi <- exp(x %*% beta_cur) / (1 + exp(x %*% beta_cur))
    
    gradient <- t(x) %*% (y - new_pi)
    hessian_ls = as.list(rep(NA, length.out = length(y)))
    
    for(i in 1:length(y)){
      hessian_ls[[i]] <- as.numeric(new_pi[i, ] * (1 - new_pi[i, ])) * tcrossprod(x[i,], x[i,])
    }
    
    hessian <- -1 * Reduce("+", hessian_ls)
    
    gradient_vec[iter,] = gradient
    
    # Change stopping criterion ?? either converges super fast or not at all..
    if(sqrt(sum(gradient^2)) < tol){
      message("Converged in", iter, "iterations.\n")
      se_beta <- sqrt(diag(solve(-hessian)))
      print(se_beta)
      break
    }
    
    # Update the solution
    beta_cur = beta_cur - solve(hessian) %*% gradient
  }
  
  return(list(solution = beta_cur, 
              beta_history = beta_history,
              gradient = gradient_vec,
              converged = (iter < max_iter),
              niter = iter,
              se_beta = se_beta))
}
```

```{r}
n_sim <- 1
sim_results <- vector("list", length = n_sim)

for (i in 1:n_sim) {
  sim_data <- gen_logit_data(n = 200,
                     beta0 = 1,
                     beta1 = 0.3)
  
  initial_guess <- c(2, 1)
  sim_results[[i]] <- newton_logit(beta = initial_guess,
                    x = sim_data$x,
                    y = sim_data$y)
  
}
```

```{r}
sim_results[[1]]$solution
```

## GLM
```{r}


mod = glm(y ~ x, data = sim_data, family = binomial)
coef(mod)

```


# 4

## Derivation of EM algorithm

- Survival times $t_1, \dots, t_n \sim \text{Exp}(\lambda)$.
- Times censored at $c_1, \dots, c_n$.
- We observe $y_i$, where $y_i = \min(t_i, c_i)$.
- Indicator $\delta_i = 1$ if $t_i \leq c_i$ (not censored), $\delta_i = 0$ if $t_i > c_i$ (censored).

We need $p(y \mid z, \theta)$ the complete data density.

$z$ represents the survival times of those who were censored (the missing data in this case) 

The exponential density function is:

$$
p(t \mid \lambda) = \frac{1}{\lambda} e^{-t/\lambda}
$$

where $E(\lambda) = \lambda$. 

Given that $t \sim \text{Exp}(\lambda)$:

$t_i = \delta_i y_i + (1 - \delta_i) z_i$

(We need the expected value of this for the E step)

and:

$$
p(y, t \mid \lambda) = \frac{1}{\lambda} e^{-t/\lambda}
$$

Thus, the complete data density is:

$$
p(y, t \mid \theta) = \frac{1}{\lambda} e^{-(\delta_i y_i + (1 - \delta_i) z_i)/\lambda}
$$

### Log-Likelihood Function

Taking the log:

$$
\log p(y, t \mid \theta) = \sum_{i=1}^{n} \log \lambda - \frac{1}{\lambda} \sum_{i=1}^{n} (\delta_i y_i + (1 - \delta_i) z_i)
$$

$$
- n \log \lambda - \frac{1}{\lambda} \sum_{i=1}^{n} (\delta_i y_i + (1 - \delta_i) z_i)
$$

Q function:

$$
Q(\lambda \mid \lambda_0) = E_z \left[ - n \log \lambda - \frac{1}{\lambda} \sum_{i=1}^{n} (\delta_i y_i + (1 - \delta_i) z_i) \right]
$$

Since $y_i$ and $\delta_i$ are already observed:

$$
- n \log \lambda - \frac{1}{\lambda} \sum_{i=1}^{n} \left( E_z[\delta_i y_i] + E_z[(1 - \delta_i) z_i] \right)
$$

For censored observations ($\delta_i = 0$), we use the memorylessness property 
of exponential distribution to get:


$E[z_i \mid z_i > y_i, \lambda] = y_i + \frac{1}{\lambda}$

Thus:

$Q(\lambda \mid \lambda_0) = - n \log \lambda - \frac{1}{\lambda} \sum_{i=1}^{n} \left( \delta_i y_i + (1 - \delta_i)(y_i + \lambda)] \right)$

Expanding:

$Q(\lambda \mid \lambda_0) = - n \log \lambda - \frac{1}{\lambda} \sum_{i=1}^{n} \left( \delta_i y_i + y_i + \lambda - \delta_i y_i - \delta_i \lambda)] \right)$

Simplifying:

$Q(\lambda \mid \lambda_0) = - n \log \lambda - \frac{1}{\lambda} \sum_{i=1}^{n} \left( \delta_i y_i + y_i + \lambda - \delta_i y_i - \delta_i \lambda)] \right)$

Factoring:

$$
Q(\lambda \mid \lambda_0) = - n \log \lambda - \frac{1}{\lambda} \sum_{i=1}^{n} (y_i + (1 - \delta_i) \lambda)
$$

$$
\! -n \log \lambda - \frac{1}{\lambda} \sum_{i=1}^{n} y_i - \sum_{i=1}^{n}(1- \delta_i)
$$

Taking the derivative of $Q$ w.r.t $\lambda$ and setting it to 0:

$$
\frac{\partial \ell}{\partial \lambda} = \frac{-n}{\lambda} + \frac{\sum_{i=1}^{n} y_i}{\lambda^2}
$$

$$
\frac{\sum_{i=1}^{n} y_i}{\lambda^2} = \frac{n}{\lambda}
$$

$$
\sum_{i=1}^{n} y_i = n \lambda
$$

M-step: $\frac{\sum_{i=1}^{n} y_i}{n} = \lambda$

## Implementation
```{r}
em_censored_data <- function(y, delta, guess, tol = 1e-12, max_iter = 100) {
  
  lambda <- guess 
  n <- length(y)
  tol_criteria <- Inf
  
  # define vectors to store elements of interest
  observed_ll = rep(NA, length = max_iter)
  lambda_hist = rep(NA, length = max_iter)
  
  for (iter in 1:max_iter) {
    
    # E-step: Expectation of censored survival times
    expected_t <- y + (1 - delta) * lambda  # E[t_i | censored]
    
    # M-step: Update lambda
    lambda_new <- sum(expected_t) / n
    
    # Use Q function to monitor convergence
    observed_ll[iter] <- -n * log(lambda_new) - sum(expected_t) / lambda_new
    
    # Stores lambda history
    lambda_hist[iter] <- lambda_new
    
    if(iter > 1){
      tol_criteria = observed_ll[iter] - observed_ll[iter - 1]
      
      # Uses squared difference between current and previous iteration of beta
      tol_criteria = (lambda_hist[iter] - lambda_hist[iter - 1])^2
    }
    
    if (tol_criteria < tol) {
      #cat("Converged in", iter, "iterations.\n")
      break
    }
    
    lambda <- lambda_new
  }
  
  return(list(solution = lambda, 
              lambda_history = lambda_hist,
              converged = (iter < max_iter),
              niter = iter))
}

```

```{r}
# Gets variance of EM parameter estimate via bootstrapping
get_em_var <- function(y, delta, lambda_estim, guess, B) {
  n <- length(y)
  bootstrap_estimates <- rep(NA, B)
  
  
  for (b in 1:B) {
    # Resamples data
    sample_indices <- sample(1:n, size = n, replace = TRUE)
    y_boot <- y[sample_indices]
    delta_boot <- delta[sample_indices]
    
    # Calculates new parameter estimate for sample data
    lambda_boot <- em_censored_data(y = y_boot, 
                                    delta = delta_boot,
                                    guess = guess)
        

    bootstrap_estimates[b] <- lambda_boot$solution
    
  }
  
  # Gets variance of the bootstrapped MLEs
  lambda_var <- var(bootstrap_estimates)
  
  return(lambda_var)
}
```


```{r}
init_guess <- 170
alpha <- 0.05

lambda_hat <- em_censored_data(y = veteran$time, 
                               delta = veteran$status, 
                               guess = init_guess)


var_lambda <- get_em_var(y = veteran$time, 
                        delta = veteran$status,
                        lambda_estim = lambda_hat$solution,
                        guess = init_guess,
                        B = 1000)


se_lambda <- sqrt(var_lambda)


fitted_lambda <- lambda_hat$solution

CI_lower <- fitted_lambda - qnorm(1 - alpha/2) * se_lambda
CI_upper <- fitted_lambda + qnorm(1 - alpha/2) * se_lambda

cat("Estimated lambda from EM:", fitted_lambda, "\n")
cat("95% CI for lambda (EM): [", CI_lower, ",", CI_upper, "]\n")
```

```{r}
aft_model <- phreg(Surv(time, status) ~ 1, data = veteran, dist = "weibull", shape = 1)
aft_model

CI_lower <- exp(aft_model$coefficients[1] - 1.96 * sqrt(aft_model$var[1, 1]))
CI_upper <- exp(aft_model$coefficients[1] + 1.96 * sqrt(aft_model$var[1, 1]))

cat("Estimated lambda from phreg:", exp(aft_model$coefficients[1]), "\n")
cat("95% CI for lambda (phreg): [", CI_lower, ",", CI_upper, "]\n")
```

The fitted $\lambda$ obtained using EM algorithm was 130.1797, with a CI of
[ 101.8725 , 158.4869 ]. I monitered convergence by comparing the 
difference between the observed likelihood at the current iteration and the 
observed likelihood at the previous iteration - if the difference was < 1e-12,
the algorithm was determined to have "converged".
The variance for the confidence interval was computed using bootstrap sampling.

Using the phreg command, the estimated lambda was 130.1797 with a CI of
[ 109.4726 , 154.8035 ]. Using our EM algorithm, we converged to the same
estimated lambda/mean survival time, but it looks like our EM confidence interval
was slightly wider.



# Extra Credit A
Fisher/expected information:

$$
I(\beta) = -E [H(\beta)]
$$

Expanding,

$$
I(\beta) = -E \left[ \left( X_i^T (\pi_i (1 - \pi_i)) X_i \right) \right]
$$

Since expectation does not depend on the observed data:

$$
I(\beta) = X_i^T (\pi_i (1 - \pi_i)) X_i
$$

Observed information:

$$
I_n(\beta) = -H(\beta) = X_i^T X_i (\pi_i (1 - \pi_i))
$$

Thus, observed and expected information are the same for logistic regression.

# Extra Credit B
Probit regression
$$
\Phi^{-1} \left( P(Y_i = 1 \mid X_i) \right) = X_i^T \beta
$$
$$
Pr[Y_i \mid X_i] - \Phi(X_i^T \beta)
$$
$$
Y_i \sim Bern(\pi_i)
$$

$$
Y_i \sim Bern(\pi_i)
$$

$$f(y_i \mid \pi_i) = \pi_i^{y_i} (1 - \pi_i)^{1 - y_i}$$

CDF for $\Phi$ :
$$
\Phi(X_i^T \beta) = \int_{-\infty}^{X_i^T \beta} \frac{1}{\sqrt{2\pi}} e^{- \frac{1}{2} z^2} \, dz
$$

## Log-Likelihood Function

The likelihood function is:

$$
L(\beta) = \prod_{i=1}^{n} \pi_i^{y_i} (1 - \pi_i)^{1 - y_i}
$$

Substituting \( \pi_i = \Phi(X_i^T \beta) \):

$$
L(\beta) = \prod_{i=1}^{n} \left( \Phi(X_i^T \beta) \right)^{y_i} \left(1 - \Phi(X_i^T \beta) \right)^{1 - y_i}
$$


Taking the log:

$$
\ell(\beta) = \sum_{i=1}^{n} \left[ y_i \log (\Phi(X_i^T \beta)) + (1 - y_i) \log (1 - \Phi(X_i^T \beta)) \right]
$$

---

## Gradient Calculation

We start with the left half first:

$$
\frac{\partial}{\partial \beta} \left[ y_i \log (\Phi(X_i^T \beta)) \right]
$$

Chain rule:

$$
\frac{\partial}{\partial \beta} [y_i \log (\Phi(X_i^T \beta))] = \left( \frac{y_i}{\Phi(X_i^T \beta)} \right) \cdot \frac{\partial}{\partial \beta} \left( \int_{-\infty}^{X_i^T \beta} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^2} dx \right)
$$

Since the derivative of the a CDF is the PDF:

$$
\frac{\partial}{\partial \beta} \Phi(X_i^T \beta) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} X_i
$$


$$
\frac{y_i}{\Phi(X_i^T \beta)} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} X_i
$$

Now for the right term:

$$
\frac{\partial}{\partial \beta} [(1 - y_i) \log (1 - \Phi(X_i^T \beta))]
$$

Applying the chain rule:

$$
(1 - y_i) \cdot \left( - \frac{1}{1 - \Phi(X_i^T \beta)} \right) \cdot \left( \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} X_i \right)
$$



---

### Full Gradient

Putting it all together:

$$
\sum_{i=1}^{n} \left[ \frac{y_i}{\Phi(X_i^T \beta)} \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} X_i - \frac{1 - y_i}{1 - \Phi(X_i^T \beta)} \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} X_i \right]
$$

Factoring:

$$
\sum_{i=1}^{n} X_i^T \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} \left[ \frac{y_i}{\Phi(X_i^T \beta)} - \frac{1 - y_i}{1 - \Phi(X_i^T \beta)} \right]
$$

## Hessian Calculation

Taking the second derivative using the product rule:

$$
\frac{\partial^2 \ell}{\partial \beta^2} = \sum_{i=1}^{n} \left( X_i^T \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} \right) \left[ \frac{\partial}{\partial \beta} \left( \frac{y_i}{\Phi(X_i^T \beta)} - \frac{1 - y_i}{1 - \Phi(X_i^T \beta)} \right) \right] + \left ( \frac{y_i}{\Phi(X_i^T \beta)} - \frac{1 - y_i}{1 - \Phi(X_i^T \beta)} \right ) \left [ X_i^T \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} X_i^T \beta X_i \right ]
$$


The first term in the Hessian calculation:

$$
\left( X_i^T \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} \right) \left[ \frac{\partial}{\partial \beta} \left( \frac{y_i}{\Phi(X_i^T \beta)} - \frac{1 - y_i}{1 - \Phi(X_i^T \beta)} \right) \right]
$$

Using the quotient rule:

$$
\frac{\partial}{\partial \beta} \left( \frac{y_i}{\Phi(X_i^T \beta)} \right) =
\frac{\Phi(X_i^T \beta) (0) - y_i \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} \right)}{\left[ \Phi(X_i^T \beta) \right]^2}
$$

$$
= - \frac{y_i}{\left[ \Phi(X_i^T \beta) \right]^2} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}}
$$

Similarly, for the second term:

$$
\frac{\partial}{\partial \beta} \left( \frac{1 - y_i}{1 - \Phi(X_i^T \beta)} \right) =
\frac{(1 - \Phi(X_i^T \beta)) (0) - (1 - y_i) \left(-\frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} \right)}{\left(1 - \Phi(X_i^T \beta)\right)^2}
$$

$$
= \frac{(1 - y_i)}{\left(1 - \Phi(X_i^T \beta)\right)^2} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}}
$$


### Full Hessian

All together:

$$
\sum_{i=1}^{n} \left[
- y_i \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}}\right) \frac{1}{\left[ \Phi(X_i^T \beta) \right]^2}
+ (1 - y_i) \left(\frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}}\right) \frac{1}{(1 - \Phi(X_i^T \beta))^2}
\right] 
$$

$$
+ \left[ \frac{y_i}{\Phi(X_i^T \beta)} - \frac{1 - y_i}{1 - \Phi(X_i^T \beta)} \right]
\left( X_i^T \frac{1}{\sqrt{2\pi}} e^{-\frac{(X_i^T \beta)^2}{2}} - X_i^T \beta X_i \right)
$$


We can see that the expected and observed information will be different because 
$y_i$ is present in the Hessian, so it's dependent on our data.

